import json
import re
from io import BytesIO
from typing import Dict, List, Optional, Tuple

import pandas as pd
import streamlit as st

st.set_page_config(page_title="Ğ­Ğ—Ğ¡ â€” Ğ´Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹", layout="wide")


# =========================
# Defaults
# =========================
DEFAULT_SHEET_ID = "1YN_8UtrZMqOTYZHaLzczwkkfocD-sS_wKrlSBmn-S50"
DEFAULT_GIDS = "2075524941"  # Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¿ÑÑ‚ÑƒÑ

DEFAULT_THEME_RULES = [
    {"theme": "ĞœĞ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ", "keywords": ["Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½", "Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶"]},
    {"theme": "Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞµÑÑĞ¸Ğ¸ / ĞĞ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ", "keywords": ["Ğ½Ğµ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚", "Ğ·Ğ°Ğ¿ÑƒÑĞº", "Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°", "ÑĞµÑÑĞ¸"]},
    {"theme": "ĞŸÑ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞµÑÑĞ¸Ğ¸", "keywords": ["Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½", "ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½"]},
    {"theme": "ĞŸĞ»Ğ°Ñ‚ĞµĞ¶Ğ¸ / Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ", "keywords": ["Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ½", "Ğ±Ğ°Ğ½ĞºĞ¾Ğ²ÑĞº", "Ğ±Ğ°Ğ»Ğ°Ğ½Ñ", "Ğ´ĞµĞ½ĞµĞ¶", "Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚"]},
    {"theme": "Ğ¡ĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ/Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ", "keywords": ["Ğ½Ğ¸Ğ·ĞºĞ°Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ", "Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾", "Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚"]},
    {"theme": "ĞÑ„Ñ„Ğ»Ğ°Ğ¹Ğ½/ÑĞµÑ‚ÑŒ/Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ", "keywords": ["Ğ½Ğµ Ğ² ÑĞµÑ‚Ğ¸", "Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿", "Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³"]},
    {"theme": "ĞŸĞ°Ñ€ĞºĞ¾Ğ²ĞºĞ°/Ğ·Ğ°Ğ½ÑÑ‚Ğ¾", "keywords": ["Ğ¿Ğ°Ñ€ĞºĞ¾Ğ²", "Ğ·Ğ°Ğ½ÑÑ‚Ğ¾", "Ğ´Ğ²Ñ", "Ğ¿Ğ´Ğ´"]},
    {"theme": "ĞšĞ¾Ğ½Ğ½ĞµĞºÑ‚Ğ¾Ñ€Ñ‹/ĞºĞ½Ğ¾Ğ¿ĞºĞ°", "keywords": ["ĞºĞ¾Ğ½Ğ½ĞµĞºÑ‚Ğ¾Ñ€", "Ğ°Ğ²Ğ°Ñ€Ğ¸Ğ¹Ğ½", "ĞºĞ½Ğ¾Ğ¿Ğº"]},
    {"theme": "Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ­Ğ—Ğ¡", "keywords": ["ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğº", "Ñ‚ĞµÑ€Ñ€Ğ¸Ñ‚Ğ¾Ñ€Ğ¸"]},
]

# ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ² "Ğ·Ğ°Ğ²Ğ¾Ğ´Ñ‹"
DEFAULT_VENDOR_MAP = {
    "Ğ•ĞŸĞ ĞĞœ": ["ĞµĞ¿Ñ€Ğ¾Ğ¼", "eprom", "e-prom", "e prom"],
    "ĞĞ¡ĞŸ": ["Ğ½ÑĞ¿", "nsp"],
}


# =========================
# Helpers (safe / robust)
# =========================
def gsheets_csv_url(sheet_id: str, gid: str) -> str:
    return f"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={gid}"


def _norm(s: str) -> str:
    return re.sub(r"\s+", " ", str(s).strip()).lower()


def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [re.sub(r"\s+", " ", str(c).strip()) for c in df.columns]
    return df


def pick_col(columns: List[str], candidates: List[str]) -> Optional[str]:
    if not columns:
        return None
    norm_cols = {_norm(c): c for c in columns}
    for cand in candidates:
        key = _norm(cand)
        if key in norm_cols:
            return norm_cols[key]
    # partial match fallback
    for cand in candidates:
        key = _norm(cand)
        for n, orig in norm_cols.items():
            if key and key in n:
                return orig
    return None


def safe_stop(msg: str, details: Optional[str] = None) -> None:
    st.error(msg)
    if details:
        with st.expander("ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸"):
            st.code(details)
    st.stop()


def read_table_from_upload(uploaded) -> pd.DataFrame:
    name = (uploaded.name or "").lower()
    data = uploaded.getvalue()
    try:
        if name.endswith(".xlsx") or name.endswith(".xls"):
            return pd.read_excel(BytesIO(data))
        return pd.read_csv(BytesIO(data), on_bad_lines="skip")
    except Exception as e:
        safe_stop("ĞĞµ ÑĞ¼Ğ¾Ğ³ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ».", str(e))
    return pd.DataFrame()  # unreachable


@st.cache_data(ttl=600, show_spinner=False)
def load_from_gsheets(sheet_id: str, gid: str) -> pd.DataFrame:
    url = gsheets_csv_url(sheet_id, gid)
    return pd.read_csv(url, on_bad_lines="skip")


def parse_dt_smart(df: pd.DataFrame, col_date: str, col_time: Optional[str]) -> pd.Series:
    """ĞŸÑ€Ğ¾Ğ±ÑƒĞµÑ‚ dayfirst=True Ğ¸ ĞµÑĞ»Ğ¸ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ²ÑÑ‘ NaT â€” Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµÑ‚ dayfirst=False."""
    if col_time:
        s = df[col_date].astype(str) + " " + df[col_time].astype(str)
    else:
        s = df[col_date].astype(str)

    dt1 = pd.to_datetime(s, dayfirst=True, errors="coerce")
    ok1 = dt1.notna().mean()

    if ok1 >= 0.5:
        return dt1

    dt2 = pd.to_datetime(s, dayfirst=False, errors="coerce")
    ok2 = dt2.notna().mean()

    return dt2 if ok2 > ok1 else dt1


def classify_theme(text: str, rules: List[Dict]) -> str:
    t = _norm(text)
    for r in rules:
        theme = (r.get("theme", "") or "").strip() or "Ğ‘ĞµĞ· Ñ‚ĞµĞ¼Ñ‹"
        kws = r.get("keywords", []) or []
        for k in kws:
            kk = _norm(k)
            if kk and kk in t:
                return theme
    return "Ğ”Ñ€ÑƒĞ³Ğ¾Ğµ"


def add_totals_crosstab(index: pd.Series, columns: pd.Series, total_name: str = "Ğ˜Ñ‚Ğ¾Ğ³Ğ¾") -> pd.DataFrame:
    idx = index.fillna("â€”").astype(str)
    col = columns.fillna("â€”").astype(str)
    ct = pd.crosstab(idx, col, dropna=False)
    ct[total_name] = ct.sum(axis=1)
    total_row = ct.sum(axis=0).to_frame().T
    total_row.index = [total_name]
    out = pd.concat([ct, total_row], axis=0).reset_index().rename(columns={"index": "Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ°"})
    return out


def normalize_vendor_to_plant(v: str, vendor_map: Dict[str, List[str]]) -> str:
    s = _norm(v)
    if not s or s == "nan":
        return "â€”"
    for plant, keys in vendor_map.items():
        for k in keys:
            kk = _norm(k)
            if kk and kk in s:
                return plant
    return "Ğ”Ñ€ÑƒĞ³Ğ¾Ğµ"


def to_excel_bytes(sheets: Dict[str, pd.DataFrame]) -> bytes:
    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as w:
        for name, sdf in sheets.items():
            sdf.to_excel(w, sheet_name=name[:31], index=False)
    return bio.getvalue()


# =========================
# UI: source
# =========================
st.title("ğŸ“Š Ğ­Ğ—Ğ¡ â€” Ğ´Ğ°ÑˆĞ±Ğ¾Ñ€Ğ´ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹ (v5)")

with st.sidebar:
    st.header("Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº")
    mode = st.radio("ĞÑ‚ĞºÑƒĞ´Ğ° Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ?", ["Google Sheets (1+ Ğ»Ğ¸ÑÑ‚Ğ¾Ğ²)", "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ» (CSV/Excel)"])

    sheet_id = DEFAULT_SHEET_ID
    gids_text = DEFAULT_GIDS
    uploaded = None

    if mode == "Google Sheets (1+ Ğ»Ğ¸ÑÑ‚Ğ¾Ğ²)":
        sheet_id = st.text_input("Google Sheet ID", value=DEFAULT_SHEET_ID)
        gids_text = st.text_input("GID Ğ»Ğ¸ÑÑ‚Ğ¾Ğ² (Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¿ÑÑ‚ÑƒÑ)", value=DEFAULT_GIDS)
        st.caption("Ğ•ÑĞ»Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ¼ĞµÑÑÑ†Ğ°Ğ¼ Ğ»ĞµĞ¶Ğ°Ñ‚ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²ĞºĞ»Ğ°Ğ´ĞºĞ°Ñ… â€” Ğ´Ğ¾Ğ±Ğ°Ğ²ÑŒ Ğ²ÑĞµ GID ÑÑĞ´Ğ°.")
        if st.button("ğŸ”„ Ğ¡Ğ±Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ ĞºÑÑˆ"):
            st.cache_data.clear()
            st.toast("ĞšÑÑˆ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½.", icon="âœ…")
    else:
        uploaded = st.file_uploader("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸ CSV Ğ¸Ğ»Ğ¸ Excel", type=["csv", "xlsx", "xls"])

    st.divider()
    st.header("Ğ¢ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)")
    rules_text = st.text_area(
        "ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ° (JSON). ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ.",
        value=json.dumps(DEFAULT_THEME_RULES, ensure_ascii=False, indent=2),
        height=220,
    )
    try:
        theme_rules = json.loads(rules_text)
        if not isinstance(theme_rules, list):
            raise ValueError("JSON Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞºĞ¾Ğ¼.")
    except Exception as e:
        st.warning(f"JSON Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» ÑĞ»Ğ¾Ğ¼Ğ°Ğ½ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´ĞµÑ„Ğ¾Ğ»Ñ‚. ({e})")
        theme_rules = DEFAULT_THEME_RULES

    st.divider()
    st.header("Ğ—Ğ°Ğ²Ğ¾Ğ´Ñ‹ (Ğ•ĞŸĞ ĞĞœ/ĞĞ¡ĞŸ)")
    vendor_map_text = st.text_area(
        "Ğ¡Ğ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ñ â†’ Ğ·Ğ°Ğ²Ğ¾Ğ´ (JSON).",
        value=json.dumps(DEFAULT_VENDOR_MAP, ensure_ascii=False, indent=2),
        height=160,
    )
    try:
        vendor_map = json.loads(vendor_map_text)
        if not isinstance(vendor_map, dict):
            raise ValueError("JSON Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ‘Ğ¼.")
    except Exception as e:
        st.warning(f"JSON Ğ·Ğ°Ğ²Ğ¾Ğ´Ğ¾Ğ² ÑĞ»Ğ¾Ğ¼Ğ°Ğ½ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´ĞµÑ„Ğ¾Ğ»Ñ‚. ({e})")
        vendor_map = DEFAULT_VENDOR_MAP


# =========================
# Load (support multi-gid)
# =========================
try:
    if mode == "Google Sheets (1+ Ğ»Ğ¸ÑÑ‚Ğ¾Ğ²)":
        gids = [g.strip() for g in str(gids_text).split(",") if g.strip()]
        if not gids:
            safe_stop("ĞĞµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ñ‹ GID Ğ»Ğ¸ÑÑ‚Ğ¾Ğ².")
        frames = []
        errors = []
        for gid in gids:
            try:
                dfi = load_from_gsheets(sheet_id, gid)
                dfi["_source_gid"] = gid
                frames.append(dfi)
            except Exception as e:
                errors.append(f"GID {gid}: {e}")
        if errors:
            st.warning("ĞĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ»Ğ¸ÑÑ‚Ñ‹ Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ»Ğ¸ÑÑŒ â€” Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ğ» Ñ Ñ‚ĞµĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ.")
            with st.expander("Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ğ¾ Ğ»Ğ¸ÑÑ‚Ğ°Ğ¼"):
                st.code("\n".join(errors))
        if not frames:
            safe_stop("ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ½Ğ¸ Ğ¾Ğ´Ğ¸Ğ½ Ğ»Ğ¸ÑÑ‚ Ğ¿Ğ¾ GID.", "\n".join(errors) if errors else None)
        raw = pd.concat(frames, ignore_index=True, sort=False)
    else:
        if not uploaded:
            st.info("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸ Ñ„Ğ°Ğ¹Ğ», Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ.")
            st.stop()
        raw = read_table_from_upload(uploaded)
        raw["_source_gid"] = "upload"
except Exception as e:
    safe_stop(
        "ĞĞµ ÑĞ¼Ğ¾Ğ³ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.",
        "Ğ§Ğ°ÑÑ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹:\n"
        "1) Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° Ğ½Ğµ Ñ€Ğ°ÑÑˆĞ°Ñ€ĞµĞ½Ğ° (Ğ½ÑƒĞ¶ĞµĞ½ public viewer)\n"
        "2) ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Sheet ID / GID\n"
        "3) Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ğ¹/Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Google\n\n"
        f"ĞÑˆĞ¸Ğ±ĞºĞ°: {e}",
    )

if raw is None or len(raw) == 0:
    safe_stop("Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° Ğ¿ÑƒÑÑ‚Ğ°Ñ Ğ¸Ğ»Ğ¸ Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ»Ğ°ÑÑŒ (0 ÑÑ‚Ñ€Ğ¾Ğº).")

df = normalize_columns(raw)

# =========================
# Column mapping (auto + manual)
# =========================
cols = df.columns.tolist()

auto_date = pick_col(cols, ["Ğ”Ğ°Ñ‚Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ", "Ğ”Ğ°Ñ‚Ğ°", "Date"])
auto_time = pick_col(cols, ["Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ", "Ğ’Ñ€ĞµĞ¼Ñ", "Time"])
auto_reason = pick_col(cols, ["ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ", "ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°", "Ğ¢ĞµĞ¼Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ"])
auto_station = pick_col(cols, ["ĞĞ¾Ğ¼ĞµÑ€ Ğ­Ğ—Ğ¡", "Ğ­Ğ—Ğ¡", "Station", "Ğ¡Ñ‚Ğ°Ğ½Ñ†Ğ¸Ñ"])
auto_vendor = pick_col(cols, ["ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒ ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¸", "ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒ", "Vendor"])
auto_note = pick_col(cols, ["ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğµ", "ĞšĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹", "Note"])
auto_id = pick_col(cols, ["â„–", "N", "No", "ĞĞ¾Ğ¼ĞµÑ€", "ID"])

with st.expander("ğŸ› ï¸ Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº"):
    st.write("Ğ•ÑĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾-Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ°Ñ…Ğ½ÑƒĞ»Ğ¾ÑÑŒ â€” Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ.")
    c1, c2 = st.columns(2)
    with c1:
        col_date = st.selectbox("ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ° Ğ”ĞĞ¢Ğ", options=cols, index=(cols.index(auto_date) if auto_date in cols else 0))
        col_time = st.selectbox("ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ° Ğ’Ğ Ğ•ĞœĞ¯ (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿ÑƒÑÑ‚Ğ¾)", options=["â€” Ğ½ĞµÑ‚ â€”"] + cols, index=(1 + cols.index(auto_time) if auto_time in cols else 0))
        col_reason = st.selectbox("ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ° ĞŸĞ Ğ˜Ğ§Ğ˜ĞĞ (ĞºĞ°Ğº Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ)", options=cols, index=(cols.index(auto_reason) if auto_reason in cols else 0))
    with c2:
        col_station = st.selectbox("ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ° ĞĞĞœĞ•Ğ  Ğ­Ğ—Ğ¡ (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿ÑƒÑÑ‚Ğ¾)", options=["â€” Ğ½ĞµÑ‚ â€”"] + cols, index=(1 + cols.index(auto_station) if auto_station in cols else 0))
        col_vendor = st.selectbox("ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ° ĞŸĞ ĞĞ˜Ğ—Ğ’ĞĞ”Ğ˜Ğ¢Ğ•Ğ›Ğ¬ (Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ²Ğ¾Ğ´Ğ¾Ğ²)", options=["â€” Ğ½ĞµÑ‚ â€”"] + cols, index=(1 + cols.index(auto_vendor) if auto_vendor in cols else 0))
        col_note = st.selectbox("ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ° ĞŸĞ Ğ˜ĞœĞ•Ğ§ĞĞĞ˜Ğ• (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿ÑƒÑÑ‚Ğ¾)", options=["â€” Ğ½ĞµÑ‚ â€”"] + cols, index=(1 + cols.index(auto_note) if auto_note in cols else 0))
    st.caption("ĞŸĞµÑ€Ğ²Ñ‹Ğµ 10 ÑÑ‚Ñ€Ğ¾Ğº:")
    st.dataframe(df.head(10), use_container_width=True)

# normalize "â€” Ğ½ĞµÑ‚ â€”"
col_time = None if col_time == "â€” Ğ½ĞµÑ‚ â€”" else col_time
col_station = None if col_station == "â€” Ğ½ĞµÑ‚ â€”" else col_station
col_vendor = None if col_vendor == "â€” Ğ½ĞµÑ‚ â€”" else col_vendor
col_note = None if col_note == "â€” Ğ½ĞµÑ‚ â€”" else col_note

if not col_date or col_date not in df.columns:
    safe_stop("ĞĞµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ°Ñ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° Ğ´Ğ°Ñ‚Ñ‹.")
if not col_reason or col_reason not in df.columns:
    safe_stop("ĞĞµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ°Ñ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ (ĞºĞ°Ğº Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ).")

# =========================
# Parse datetime + derived
# =========================
df["_dt"] = parse_dt_smart(df, col_date, col_time)

if df["_dt"].isna().all():
    safe_stop(
        "ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ñ€Ğ°ÑĞ¿Ğ°Ñ€ÑĞ¸Ñ‚ÑŒ Ğ´Ğ°Ñ‚Ñ‹/Ğ²Ñ€ĞµĞ¼Ñ (Ğ²ÑĞµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ»Ğ¸ Ğ¿ÑƒÑÑ‚Ñ‹Ğ¼Ğ¸).",
        "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ°Ñ‚Ñ‹/Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ.\n"
        "Ğ•ÑĞ»Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ»Ğ¸ÑÑ‚Ğ°Ñ… â€” ÑƒĞ±ĞµĞ´Ğ¸ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¹.\n"
        "Ğ¡Ğ¾Ğ²ĞµÑ‚: Ğ² Google Sheets Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²ÑŒ Ñ‚Ğ¸Ğ¿ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ° 'Ğ”Ğ°Ñ‚Ğ°' Ğ¸ 'Ğ’Ñ€ĞµĞ¼Ñ'."
    )

df["_week_start"] = df["_dt"].dt.to_period("W-MON").dt.start_time
df["_week_label"] = df["_week_start"].dt.strftime("%Y-%m-%d")
df["_month"] = df["_dt"].dt.to_period("M")  # Period[M]
df["_month_label"] = df["_month"].astype(str)

# Ğ—Ğ°Ğ²Ğ¾Ğ´ (Ğ•ĞŸĞ ĞĞœ/ĞĞ¡ĞŸ)
if col_vendor and col_vendor in df.columns:
    df["Ğ—Ğ°Ğ²Ğ¾Ğ´"] = df[col_vendor].astype(str).apply(lambda x: normalize_vendor_to_plant(x, vendor_map))
else:
    df["Ğ—Ğ°Ğ²Ğ¾Ğ´"] = "â€”"

# Ğ¢ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾; ĞĞ• Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñƒ)
df["Ğ¢ĞµĞ¼Ğ°"] = df[col_reason].astype(str).apply(lambda x: classify_theme(x, theme_rules))

# =========================
# Filters
# =========================
st.subheader("Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹")
c1, c2, c3, c4, c5 = st.columns([1.2, 1.2, 1.2, 1.4, 1.2])

latest_week = df["_week_label"].dropna().max()

with c1:
    period_mode = st.radio("ĞŸĞµÑ€Ğ¸Ğ¾Ğ´", ["ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ÑÑ Ğ½ĞµĞ´ĞµĞ»Ñ", "Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ½ĞµĞ´ĞµĞ»Ğ¸", "Ğ”Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ´Ğ°Ñ‚"], horizontal=False)

with c2:
    if period_mode == "Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ½ĞµĞ´ĞµĞ»Ğ¸":
        week = st.selectbox("ĞĞµĞ´ĞµĞ»Ñ (Ğ¿Ğ¾Ğ½ĞµĞ´ĞµĞ»ÑŒĞ½Ğ¸Ğº)", sorted(df["_week_label"].dropna().unique())[::-1], index=0)
    else:
        week = latest_week

with c3:
    if period_mode == "Ğ”Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ´Ğ°Ñ‚":
        min_dt = df["_dt"].min().date()
        max_dt = df["_dt"].max().date()
        start_date = st.date_input("Ğ¡ Ğ´Ğ°Ñ‚Ñ‹", value=min_dt)
        end_date = st.date_input("ĞŸĞ¾ Ğ´Ğ°Ñ‚Ñƒ", value=max_dt)
    else:
        start_date, end_date = None, None

with c4:
    plant_filter = st.multiselect(
        "Ğ—Ğ°Ğ²Ğ¾Ğ´",
        options=sorted(df["Ğ—Ğ°Ğ²Ğ¾Ğ´"].dropna().unique()),
        default=["Ğ•ĞŸĞ ĞĞœ", "ĞĞ¡ĞŸ"] if set(["Ğ•ĞŸĞ ĞĞœ", "ĞĞ¡ĞŸ"]).issubset(set(df["Ğ—Ğ°Ğ²Ğ¾Ğ´"].unique())) else [],
        placeholder="Ğ’ÑĞµ Ğ·Ğ°Ğ²Ğ¾Ğ´Ñ‹",
    )

with c5:
    reason_filter = st.multiselect(
        "ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ° (ĞºĞ°Ğº Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ)",
        options=sorted(df[col_reason].dropna().astype(str).unique()),
        default=[],
        placeholder="Ğ’ÑĞµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹",
    )

# apply filters
fdf = df
if period_mode in ("ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ÑÑ Ğ½ĞµĞ´ĞµĞ»Ñ", "Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ½ĞµĞ´ĞµĞ»Ğ¸") and week:
    fdf = fdf[fdf["_week_label"] == week]

if period_mode == "Ğ”Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ´Ğ°Ñ‚":
    start = pd.to_datetime(start_date)
    end = pd.to_datetime(end_date) + pd.Timedelta(days=1)
    fdf = fdf[(fdf["_dt"] >= start) & (fdf["_dt"] < end)]

if plant_filter:
    fdf = fdf[fdf["Ğ—Ğ°Ğ²Ğ¾Ğ´"].isin(plant_filter)]

if reason_filter:
    fdf = fdf[fdf[col_reason].astype(str).isin(reason_filter)]

# =========================
# KPIs
# =========================
k1, k2, k3, k4 = st.columns(4)
total = int(len(fdf))
uniq_station = int(fdf[col_station].nunique()) if col_station else 0
k1.metric("ĞĞ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹", f"{total}")
k2.metric("Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ­Ğ—Ğ¡", f"{uniq_station}" if col_station else "â€”")
k3.metric("Ğ¢Ğ¾Ğ¿-Ğ·Ğ°Ğ²Ğ¾Ğ´", fdf["Ğ—Ğ°Ğ²Ğ¾Ğ´"].value_counts().index[0] if total else "â€”")
k4.metric("Ğ¢Ğ¾Ğ¿-Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°", fdf[col_reason].value_counts().index[0] if total else "â€”")

st.divider()

# =========================
# Correct breakdown: Reason x Plant
# =========================
st.markdown("#### Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²ĞºĞ° Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°Ğ¼ Ã— Ğ·Ğ°Ğ²Ğ¾Ğ´ (Ğ•ĞŸĞ ĞĞœ / ĞĞ¡ĞŸ)")
# rows = ĞºĞ°Ğº Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ, columns = Ğ·Ğ°Ğ²Ğ¾Ğ´
reason_plant = pd.crosstab(
    fdf[col_reason].fillna("â€”").astype(str),
    fdf["Ğ—Ğ°Ğ²Ğ¾Ğ´"].fillna("â€”").astype(str),
    dropna=False,
).reset_index().rename(columns={col_reason: "ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°"})

# ensure columns order
for col in ["Ğ•ĞŸĞ ĞĞœ", "ĞĞ¡ĞŸ", "Ğ”Ñ€ÑƒĞ³Ğ¾Ğµ", "â€”"]:
    if col in reason_plant.columns:
        pass
st.dataframe(reason_plant, use_container_width=True, hide_index=True)

st.divider()

# =========================
# Optional: Themes (still useful)
# =========================
with st.expander("Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾: ÑĞ²Ğ¾Ğ´ĞºĞ° Ğ¿Ğ¾ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ°Ğ¼ (Ğ°Ğ²Ñ‚Ğ¾-ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ)"):
    theme_counts = fdf["Ğ¢ĞµĞ¼Ğ°"].value_counts().rename_axis("Ğ¢ĞµĞ¼Ğ°").reset_index(name="ĞĞ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ")
    st.dataframe(theme_counts, use_container_width=True, hide_index=True)
    if len(theme_counts):
        st.bar_chart(theme_counts.set_index("Ğ¢ĞµĞ¼Ğ°")["ĞĞ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ"])

# =========================
# Monthly summary 2024-2025 across ALL LOADED DATA
# =========================
st.markdown("#### Ğ’ÑĞµ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¼ĞµÑÑÑ†Ğ°Ğ¼ (2024â€“2025) â€” Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ»Ğ¸ÑÑ‚Ğ°Ğ¼")
df_2425 = df[df["_dt"].dt.year.isin([2024, 2025])].copy()
month_range = pd.period_range("2024-01", "2025-12", freq="M")

monthly = (
    df_2425.dropna(subset=["_dt"])
          .groupby(df_2425["_dt"].dt.to_period("M"))
          .size()
          .reindex(month_range, fill_value=0)
)

monthly_table = pd.DataFrame([monthly.values], columns=monthly.index.astype(str))
monthly_table.insert(0, "ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ", "ĞĞ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ")
st.dataframe(monthly_table, use_container_width=True, hide_index=True)

st.caption("Ğ•ÑĞ»Ğ¸ Ñ‚ÑƒÑ‚ Ğ½ÑƒĞ»Ğ¸ â€” Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ Ğ² Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… GID Ğ½ĞµÑ‚ Ğ´Ğ°Ñ‚ 2024â€“2025 (Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ñ‹ Ğ½Ğµ Ğ¿Ğ°Ñ€ÑÑÑ‚ÑÑ). ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒ GID Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ°Ñ‚Ñ‹.")

st.divider()

# =========================
# Top-5 stations (filtered)
# =========================
st.markdown("#### Ğ¢ĞĞŸ-5 Ğ­Ğ—Ğ¡ Ğ¿Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸ÑĞ¼ (Ğ² Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğµ)")
top5 = pd.DataFrame()
if not col_station:
    st.info("ĞšĞ¾Ğ»Ğ¾Ğ½ĞºĞ° ĞĞ¾Ğ¼ĞµÑ€ Ğ­Ğ—Ğ¡ Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ° â€” Ğ¢ĞĞŸ-5 Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½.")
else:
    top5 = (
        fdf.groupby(col_station)
           .size()
           .sort_values(ascending=False)
           .head(5)
           .rename("ĞĞ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ")
           .reset_index()
    )
    # Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ²Ğ¾Ğ´ (ÑĞ°Ğ¼Ñ‹Ğ¹ Ñ‡Ğ°ÑÑ‚Ñ‹Ğ¹ Ğ¿Ğ¾ ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¸)
    plant_map = (
        fdf.groupby(col_station)["Ğ—Ğ°Ğ²Ğ¾Ğ´"]
           .agg(lambda s: s.dropna().astype(str).mode().iloc[0] if len(s.dropna()) else "â€”")
    )
    top5["Ğ—Ğ°Ğ²Ğ¾Ğ´"] = top5[col_station].map(plant_map)

    st.dataframe(top5, use_container_width=True, hide_index=True)
    st.bar_chart(top5.set_index(col_station)["ĞĞ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ"])

st.divider()

# =========================
# Raw data (filtered)
# =========================
st.markdown("#### Ğ¡Ñ‹Ñ€Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (Ğ¿Ğ¾ÑĞ»Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²)")
show_cols: List[str] = []
for c in [auto_id, col_date, col_time, col_reason, "Ğ—Ğ°Ğ²Ğ¾Ğ´", col_station, col_vendor, col_note, "_source_gid"]:
    if c and c in fdf.columns and c not in show_cols:
        show_cols.append(c)
if not show_cols:
    show_cols = [c for c in fdf.columns if not str(c).startswith("_")]

display_df = fdf.sort_values("_dt", ascending=False) if "_dt" in fdf.columns else fdf
st.dataframe(display_df[show_cols], use_container_width=True, hide_index=True)

# =========================
# Export
# =========================
st.markdown("### Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚")
d1, d2 = st.columns([1, 1])

with d1:
    st.download_button(
        "â¬‡ï¸ CSV (Ğ¿Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ğ¼)",
        data=display_df[show_cols].to_csv(index=False).encode("utf-8-sig"),
        file_name="ezs_filtered.csv",
        mime="text/csv",
    )

with d2:
    sheets: Dict[str, pd.DataFrame] = {
        "reason_x_plant_filtered": reason_plant,
        "monthly_2024_2025_all": monthly_table,
        "filtered_raw": display_df[show_cols],
    }
    if len(top5):
        sheets["top5_filtered"] = top5

    st.download_button(
        "â¬‡ï¸ Excel (ÑĞ²Ğ¾Ğ´ĞºĞ°)",
        data=to_excel_bytes(sheets),
        file_name="ezs_dashboard_export.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    )

st.caption("v5: Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° â€” Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° Ñ‚ĞµĞ¿ĞµÑ€ÑŒ 'ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ° (ĞºĞ°Ğº Ğ² Google) Ã— Ğ·Ğ°Ğ²Ğ¾Ğ´ (Ğ•ĞŸĞ ĞĞœ/ĞĞ¡ĞŸ)'. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²ĞºĞ»Ğ°Ğ´Ğ¾Ğº (Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ GID).")
